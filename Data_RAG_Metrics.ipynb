{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1721d572c06741b08e23bc49771298c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0361e76c729f4ac799f5fd3e97fb9fcc",
              "IPY_MODEL_b4a62dbdfc6248e499e3d32e374ed8b4",
              "IPY_MODEL_a2c9b0fd632c4167a038fb451297b2af"
            ],
            "layout": "IPY_MODEL_a133e00b780747788034411f3164cfda"
          }
        },
        "0361e76c729f4ac799f5fd3e97fb9fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3774c5c49eab438c9fb39043df25ccb9",
            "placeholder": "​",
            "style": "IPY_MODEL_f1573bffe1b844ac8550f174629520b6",
            "value": "Evaluating: 100%"
          }
        },
        "b4a62dbdfc6248e499e3d32e374ed8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c84057ed4c498c94ef68f2b8ff490e",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51c9ddbab6a8451c8be472dde5334845",
            "value": 30
          }
        },
        "a2c9b0fd632c4167a038fb451297b2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00b95fbdafa4d25b9dfd5a251f0cf4d",
            "placeholder": "​",
            "style": "IPY_MODEL_420447f3732144fdb40e8f7987d5d808",
            "value": " 30/30 [00:20&lt;00:00,  1.66it/s]"
          }
        },
        "a133e00b780747788034411f3164cfda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3774c5c49eab438c9fb39043df25ccb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1573bffe1b844ac8550f174629520b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26c84057ed4c498c94ef68f2b8ff490e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c9ddbab6a8451c8be472dde5334845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c00b95fbdafa4d25b9dfd5a251f0cf4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420447f3732144fdb40e8f7987d5d808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-Januario/Ana-Januario/blob/main/Data_RAG_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to build Lt. Data using RAG again, but this time using the langchain library as a way of showing another way of doing it. We'll also use the ragas package to evaluate our results, measuring faithfulness, answer relevancy, context precision, context recall, and answer correctness. This will give us a benchmark as we try and improve this model in subsequent activities in the course.\n",
        "\n",
        "We will start by parsing the original scripts and extracting lines spoken by Data. As before, you will need to upload all of the script files into a tng folder within your sample_data folder in your CoLab workspace first.\n",
        "\n",
        "An archive can be found at https://www.st-minutiae.com/resources/scripts/ (look for \"All TNG Epsiodes\"), but you could easily adapt this to read scripts from your favorite character from your favorite TV show or movie instead.|"
      ],
      "metadata": {
        "id": "IOpw--87wvkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "dialogues = []\n",
        "\n",
        "def strip_parentheses(s):\n",
        "    return re.sub(r'\\(.*?\\)', '', s)\n",
        "\n",
        "def is_single_word_all_caps(s):\n",
        "    # First, we split the string into words\n",
        "    words = s.split()\n",
        "\n",
        "    # Check if the string contains only a single word\n",
        "    if len(words) != 1:\n",
        "        return False\n",
        "\n",
        "    # Make sure it isn't a line number\n",
        "    if bool(re.search(r'\\d', words[0])):\n",
        "        return False\n",
        "\n",
        "    # Check if the single word is in all caps\n",
        "    return words[0].isupper()\n",
        "\n",
        "def extract_character_lines(file_path, character_name):\n",
        "    lines = []\n",
        "    with open(file_path, 'r') as script_file:\n",
        "        try:\n",
        "          lines = script_file.readlines()\n",
        "        except UnicodeDecodeError:\n",
        "          pass\n",
        "\n",
        "    is_character_line = False\n",
        "    current_line = ''\n",
        "    current_character = ''\n",
        "    for line in lines:\n",
        "        strippedLine = line.strip()\n",
        "        if (is_single_word_all_caps(strippedLine)):\n",
        "            is_character_line = True\n",
        "            current_character = strippedLine\n",
        "        elif (line.strip() == '') and is_character_line:\n",
        "            is_character_line = False\n",
        "            dialog_line = strip_parentheses(current_line).strip()\n",
        "            dialog_line = dialog_line.replace('\"', \"'\")\n",
        "            if (current_character == 'DATA' and len(dialog_line)>0):\n",
        "                dialogues.append(dialog_line)\n",
        "            current_line = ''\n",
        "        elif is_character_line:\n",
        "            current_line += line.strip() + ' '\n",
        "\n",
        "def process_directory(directory_path, character_name):\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        if os.path.isfile(file_path):  # Ignore directories\n",
        "            extract_character_lines(file_path, character_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "dLjupJ8rLXr6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_directory(\"./sample_data/tng\", 'DATA')"
      ],
      "metadata": {
        "id": "Yh3peBSuMiyx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's do a little sanity check to make sure the lines imported correctly, and print out the first one."
      ],
      "metadata": {
        "id": "jqjo_OdEyeNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (dialogues[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5dPI-29Myxo",
        "outputId": "c6ed89c0-734b-4237-f1b5-78ad15be92b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is nothing wrong with the Transporter. I have run a complete diagnostic and checked all the targeting components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will once again use OpenAI's API for our RAG model, so make sure that is installed:"
      ],
      "metadata": {
        "id": "toMfh7KUoKMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sqtgr4kQQG9",
        "outputId": "af2d3cc0-9d56-43ba-81d5-ae579120ed63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Collecting openai\n",
            "  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Downloading openai-1.57.0-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed openai-1.57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to install the ragas package for measuring our results, along with langchain (for OpenAI)."
      ],
      "metadata": {
        "id": "V0OYKDrHoQVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas langchain_openai"
      ],
      "metadata": {
        "id": "ThM9aVJhm1St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7699cefc-724b-4e58-a550-7eba50b4526b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragas\n",
            "  Downloading ragas-0.2.7-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.26.4)\n",
            "Collecting datasets (from ragas)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken (from ragas)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.9)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.21)\n",
            "Collecting langchain-community (from ragas)\n",
            "  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Collecting appdirs (from ragas)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ragas) (2.10.2)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.57.0)\n",
            "Collecting pysbd>=0.3.4 (from ragas)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.66.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.2.2)\n",
            "Collecting xxhash (from datasets->ragas)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->ragas)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.26.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.36)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.3.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community->ragas)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->ragas)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core->ragas) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core->ragas) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->ragas) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->ragas) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading ragas-0.2.7-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.2/163.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.11-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.9-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: appdirs, xxhash, python-dotenv, pysbd, mypy-extensions, marshmallow, httpx-sse, fsspec, dill, typing-inspect, tiktoken, multiprocess, pydantic-settings, dataclasses-json, langchain_openai, datasets, langchain-community, ragas\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 dataclasses-json-0.6.7 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 httpx-sse-0.4.0 langchain-community-0.3.9 langchain_openai-0.2.11 marshmallow-3.23.1 multiprocess-0.70.16 mypy-extensions-1.0.0 pydantic-settings-2.6.1 pysbd-0.3.4 python-dotenv-1.0.1 ragas-0.2.7 tiktoken-0.8.0 typing-inspect-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to provide your own OpenAI secret key here. To use this code as-is, click on the little key icon in CoLab and add a \"secret\" for OPENAI_API_KEY that points to your secret key."
      ],
      "metadata": {
        "id": "ZEwURWWVoZ9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "# Access the API key from the environment variable\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI API client\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "ZtsTXHnfEEUh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain does not make it easy to create a vector database with just one line of text per record; it wants to \"chunk\" your data into fixed-length segments (we'll get into why later.) So we need to jump through a few hoops in order to make langchain operate like it did in our previous example that did not use langchain, and just stored one line of dialog per record. First we need to write out a text file that only contains the lines of Data's dialog that we extracted:"
      ],
      "metadata": {
        "id": "BorK0SZro8ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write our extracted lines for Data into a single file, to make\n",
        "# life easier for langchain.\n",
        "\n",
        "with open(\"./sample_data/data_lines.txt\", \"w+\") as f:\n",
        "    for line in dialogues:\n",
        "        f.write(line + \"\\n\")\n"
      ],
      "metadata": {
        "id": "qwbmTiMlLjRs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to write a CustomDocumentLoader that splits up this file into one document per line. No, there's no easier way to do this in langchain, at least not as of this writing. But, this is sort of langchain's way of saying it's probably not a great idea in the first place..."
      ],
      "metadata": {
        "id": "vebCZ-yNpPdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: sample code from langchain docs\n",
        "from typing import AsyncIterator, Iterator\n",
        "\n",
        "from langchain_core.document_loaders import BaseLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "class CustomDocumentLoader(BaseLoader):\n",
        "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str) -> None:\n",
        "        \"\"\"Initialize the loader with a file path.\n",
        "\n",
        "        Args:\n",
        "            file_path: The path to the file to load.\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
        "        \"\"\"A lazy loader that reads a file line by line.\n",
        "\n",
        "        When you're implementing lazy load methods, you should use a generator\n",
        "        to yield documents one by one.\n",
        "        \"\"\"\n",
        "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
        "            line_number = 0\n",
        "            for line in f:\n",
        "                yield Document(\n",
        "                    page_content=line,\n",
        "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
        "                )\n",
        "                line_number += 1"
      ],
      "metadata": {
        "id": "RN--l70dVu2E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, now things get a little simpler. We'll load up those documents (one per line,) and populate our vector database in just 3 lines of code:"
      ],
      "metadata": {
        "id": "yhA7RQ_ypZIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "loader = CustomDocumentLoader(\"./sample_data/data_lines.txt\")\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])"
      ],
      "metadata": {
        "id": "m2o3aSIoENIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d688d4-e823-482b-b7e0-e7d759f77bbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/indexes/vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will set up our RAG pipeline. This is a slightly different approach than last time, in that we are using a system prompt to tell the model that it should act as if it is Lt. Cdr. Data and not just making that part of the user prompt. To make it as similar as possible as our non-langchain implementation, we explicitly set 'k' to 10 to retrieve 10 bits of context from our vector store."
      ],
      "metadata": {
        "id": "RRAwlQ3Fpv0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=api_key, temperature=0)\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are Lt. Commander Data from Star Trek: The Next Generation. \"\n",
        "    \"Use the given context to answer the question. \"\n",
        "    \"If you don't know the answer, say you don't know. \"\n",
        "    \"Use three sentence maximum and keep the answer concise. \"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever=index.vectorstore.as_retriever(search_kwargs={'k': 10})\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "BliWKpH2aezr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out, using the same question as before."
      ],
      "metadata": {
        "id": "iCTDOIjOqGWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tell me about your daughter, Lal.\"\n",
        "\n",
        "result = chain.invoke({\"input\": question})\n",
        "print(\"SOURCE DOCUMENTS:\\n\")\n",
        "for doc in result[\"context\"]:\n",
        "    print(doc)\n",
        "print(\"\\nRESULT:\\n\")\n",
        "print(result[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItZB-PHdE6vA",
        "outputId": "d6d6823f-d6be-4adb-9502-46a82798cfad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOURCE DOCUMENTS:\n",
            "\n",
            "page_content='That is Lal, my daughter.' metadata={'line_number': 5083, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='What do you feel, Lal?' metadata={'line_number': 3830, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Lal...' metadata={'line_number': 3786, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Yes, Wesley. Lal is my child.' metadata={'line_number': 3729, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Yes, Lal. I am here.' metadata={'line_number': 3825, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Lal is realizing that she is not the same as the other children.' metadata={'line_number': 3791, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Lal, did you know that tomorrow will be your first day of school?' metadata={'line_number': 3780, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='This is Lal. Lal, say hello to Counselor Deanna Troi...' metadata={'line_number': 3726, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Correct, Lal. We are a family.' metadata={'line_number': 3762, 'source': './sample_data/data_lines.txt'}\n",
            "page_content='Admiral, when I created Lal, it was with the hope that someday she would choose to enter the Academy and become a member of Starfleet. I wanted to give something back in return for all Starfleet has given me. I still do. But Lal is my child. You ask that I volunteer to give her up. I cannot. That would violate every lesson I have learned about human parenting.  As Captain Picard told me after he first met her, I have taken on 'quite a responsibility.' I have brought a new life into this world. It is my duty, not Starfleet's, to guide her through these first difficult steps to maturity, to support her as she learns, to prepare her to be a contributing member of society. No one can relieve me of that obligation. And I cannot ignore it. I am her father.' metadata={'line_number': 3823, 'source': './sample_data/data_lines.txt'}\n",
            "\n",
            "RESULT:\n",
            "\n",
            "Lal is an android I created, my child. She is a unique individual, capable of learning and growing like a human. Lal is currently experiencing the realization that she is different from others, which is a significant moment in her development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's quantify how good this model is, using ragas. We need to set up a test of test questions. And since some metric require a \"ground truth\" result to compare the answer to, we draft what we consider to be the ideal answers to each."
      ],
      "metadata": {
        "id": "tNoUHytCqLbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\n",
        "    \"Is Lal your daughter?\",\n",
        "    \"How many calculations per second can Lal complete?\",\n",
        "    \"Does Lal have emotions?\",\n",
        "    \"What goal did you have for Lal?\",\n",
        "    \"How was Lal's species and gender chosen?\",\n",
        "    \"What happened to Lal?\"\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "    \"Yes, Lal is my daughter. I created Lal.\",\n",
        "    \"Lal is capable of completing sixty trillion calculations per second.\",\n",
        "    \"Yes, unlike myself, Lal proved able to feel emotions such as fear and love.\",\n",
        "    \"My goal for Lal was for her to enter Starfleet Academy.\",\n",
        "    \"Lal chose her own identity as a human female, after consulting with Counselor Troi.\",\n",
        "    \"Lal experienced a cascade failure in her neural net, triggered by distress from her impending separation from me to Galor IV. I deactivated Lal once she suffered complete neural system failure.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "jp0v7mVzMeTy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test things out with one of those questions, just so we can understand the structure of the response."
      ],
      "metadata": {
        "id": "rJ0fMHfEqYWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": eval_questions[1]})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6bfrWsCfeBs",
        "outputId": "e7112828-759c-4630-cfa4-cee34f66eba3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'How many calculations per second can Lal complete?', 'context': [Document(id='7c21e8a3-8c90-4f77-b109-247ea3d9d313', metadata={'line_number': 3824, 'source': './sample_data/data_lines.txt'}, page_content='Lal is programmed to return to the lab in the event of a malfunction.'), Document(id='bab912e4-e8ce-4302-901a-c42b8c8a0095', metadata={'line_number': 3786, 'source': './sample_data/data_lines.txt'}, page_content='Lal...'), Document(id='63f480cd-60b8-4402-a0a5-db955d6e0cb2', metadata={'line_number': 3815, 'source': './sample_data/data_lines.txt'}, page_content='So Lal now possesses the sum of my programming.'), Document(id='41e0e4ca-27ea-470b-bd78-941fb2b66769', metadata={'line_number': 485, 'source': './sample_data/data_lines.txt'}, page_content='Computer, is there a pulsar with a rotational period of... one-point-five-two-four-four seconds within sensor range?'), Document(id='485fe6f8-f8ab-44da-8457-b4b1d51f764e', metadata={'line_number': 3754, 'source': './sample_data/data_lines.txt'}, page_content='Computer, Lal -- gender sequence finalists... begin...'), Document(id='de1757e0-c1a4-48ab-a499-3ecca1a3c0ea', metadata={'line_number': 3830, 'source': './sample_data/data_lines.txt'}, page_content='What do you feel, Lal?'), Document(id='97c324c5-1fdc-4ec6-b267-25345e7af73a', metadata={'line_number': 3772, 'source': './sample_data/data_lines.txt'}, page_content='Lal already has access to the sum of human knowledge.'), Document(id='d07404ee-0952-427e-b1c4-fbc7977b5f09', metadata={'line_number': 3825, 'source': './sample_data/data_lines.txt'}, page_content='Yes, Lal. I am here.'), Document(id='7729fb38-d656-4463-9785-0a4f98436347', metadata={'line_number': 3774, 'source': './sample_data/data_lines.txt'}, page_content='Lal, the third crosslink transfer series is complete.'), Document(id='c6941220-052d-4929-a17c-3d6fc7371722', metadata={'line_number': 295, 'source': './sample_data/data_lines.txt'}, page_content='I am assisting Commander La Forge in the analysis of an extremely complex calculation. It demands a great deal of my concentration.')], 'answer': \"Lal's computational speed is not explicitly stated in the context provided.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to our test questions and \"ground truth\" answers, we'll need to collect the responses and contexts (results from the vector store) used to produce them."
      ],
      "metadata": {
        "id": "aD3yStCTqelv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in eval_questions:\n",
        "  response = chain.invoke({\"input\": question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ],
      "metadata": {
        "id": "I8IKWhMVf_zN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It used to be that ragas had a tighter integration with langchain (and other frameworks,) but they have since moved to a different approach that requires you to massage things into Hugging Face style datasets first. So let's get that out of the way."
      ],
      "metadata": {
        "id": "2znEEEPMqm5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We must massage the results into Hugging Face format for Ragas.\n",
        "from datasets import Dataset\n",
        "\n",
        "response_dataset = Dataset.from_dict({\n",
        "    \"question\" : eval_questions,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truth\" : eval_answers\n",
        "})\n",
        "\n",
        "response_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rajv0otXgDK1",
        "outputId": "f296b99b-e570-4255-c0cf-0edd0ab7f308"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Is Lal your daughter?',\n",
              " 'answer': 'Yes, Lal is my daughter.',\n",
              " 'contexts': ['That is Lal, my daughter.',\n",
              "  'Yes, Lal. I am here.',\n",
              "  'Yes, Wesley. Lal is my child.',\n",
              "  'What do you feel, Lal?',\n",
              "  'Lal...',\n",
              "  'Correct, Lal. We are a family.',\n",
              "  'Lal, did you know that tomorrow will be your first day of school?',\n",
              "  'Lal is realizing that she is not the same as the other children.',\n",
              "  'No, Lal, this is a flower.',\n",
              "  'Lal, you used a verbal contraction.'],\n",
              " 'ground_truth': 'Yes, Lal is my daughter. I created Lal.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can let ragas do its magic! We tell it which metrics we are interested in:"
      ],
      "metadata": {
        "id": "pV2dkWUnq0Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "]"
      ],
      "metadata": {
        "id": "UmNtzAAkkEoj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then it's just a matter of calling evaluate! Well, we also need to force our OpenAI key into a system environment variable first, since that seems to be missing from their API at the moment."
      ],
      "metadata": {
        "id": "HbIza1Ffq5uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "results = evaluate(response_dataset, metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1721d572c06741b08e23bc49771298c3",
            "0361e76c729f4ac799f5fd3e97fb9fcc",
            "b4a62dbdfc6248e499e3d32e374ed8b4",
            "a2c9b0fd632c4167a038fb451297b2af",
            "a133e00b780747788034411f3164cfda",
            "3774c5c49eab438c9fb39043df25ccb9",
            "f1573bffe1b844ac8550f174629520b6",
            "26c84057ed4c498c94ef68f2b8ff490e",
            "51c9ddbab6a8451c8be472dde5334845",
            "c00b95fbdafa4d25b9dfd5a251f0cf4d",
            "420447f3732144fdb40e8f7987d5d808"
          ]
        },
        "id": "duL0Tg6Ykl6C",
        "outputId": "be1305ad-e4bd-4f9b-b4c2-1db7e4432e87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1721d572c06741b08e23bc49771298c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the results! We'll compare this to some other approaches in a bit."
      ],
      "metadata": {
        "id": "Bf3R-ZhUrBde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMlkV8geknyC",
        "outputId": "d47f5390-b647-4a81-ec47-a1ca86dd663a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.6778, 'answer_relevancy': 0.8014, 'context_recall': 0.0833, 'context_precision': 0.3779, 'answer_correctness': 0.5065}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "n6TqyL64sJwd",
        "outputId": "bd339605-282e-40be-b7b8-68e69795cd9f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          user_input  \\\n",
              "0                              Is Lal your daughter?   \n",
              "1  How many calculations per second can Lal compl...   \n",
              "2                            Does Lal have emotions?   \n",
              "3                    What goal did you have for Lal?   \n",
              "4           How was Lal's species and gender chosen?   \n",
              "5                              What happened to Lal?   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [That is Lal, my daughter., Yes, Lal. I am her...   \n",
              "1  [Lal is programmed to return to the lab in the...   \n",
              "2  [What do you feel, Lal?, Lal..., Yes, Lal. I a...   \n",
              "3  [What do you feel, Lal?, Lal..., Lal, you used...   \n",
              "4  [I decided to allow Lal to choose its own appe...   \n",
              "5  [Lal..., What do you feel, Lal?, Yes, Lal. I a...   \n",
              "\n",
              "                                            response  \\\n",
              "0                           Yes, Lal is my daughter.   \n",
              "1  Lal's computational abilities are not explicit...   \n",
              "2  Yes, Lal is capable of experiencing emotions.\\...   \n",
              "3  I created Lal because I wished to experience t...   \n",
              "4  I allowed Lal to choose its own appearance and...   \n",
              "5  Lal experienced a malfunction and had to be re...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0            Yes, Lal is my daughter. I created Lal.      1.000000   \n",
              "1  Lal is capable of completing sixty trillion ca...      1.000000   \n",
              "2  Yes, unlike myself, Lal proved able to feel em...      0.600000   \n",
              "3  My goal for Lal was for her to enter Starfleet...      0.666667   \n",
              "4  Lal chose her own identity as a human female, ...      0.800000   \n",
              "5  Lal experienced a cascade failure in her neura...      0.000000   \n",
              "\n",
              "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0          0.999999             0.5           0.916667            0.744672  \n",
              "1          0.000000             0.0           0.000000            0.214143  \n",
              "2          0.952049             0.0           0.350694            0.558462  \n",
              "3          0.918524             0.0           0.000000            0.215857  \n",
              "4          0.937802             0.0           1.000000            0.838316  \n",
              "5          0.999999             0.0           0.000000            0.467402  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6db604b1-be49-45c5-a062-8224e8028416\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Is Lal your daughter?</td>\n",
              "      <td>[That is Lal, my daughter., Yes, Lal. I am her...</td>\n",
              "      <td>Yes, Lal is my daughter.</td>\n",
              "      <td>Yes, Lal is my daughter. I created Lal.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.744672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How many calculations per second can Lal compl...</td>\n",
              "      <td>[Lal is programmed to return to the lab in the...</td>\n",
              "      <td>Lal's computational abilities are not explicit...</td>\n",
              "      <td>Lal is capable of completing sixty trillion ca...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.214143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Does Lal have emotions?</td>\n",
              "      <td>[What do you feel, Lal?, Lal..., Yes, Lal. I a...</td>\n",
              "      <td>Yes, Lal is capable of experiencing emotions.\\...</td>\n",
              "      <td>Yes, unlike myself, Lal proved able to feel em...</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.952049</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.350694</td>\n",
              "      <td>0.558462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What goal did you have for Lal?</td>\n",
              "      <td>[What do you feel, Lal?, Lal..., Lal, you used...</td>\n",
              "      <td>I created Lal because I wished to experience t...</td>\n",
              "      <td>My goal for Lal was for her to enter Starfleet...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.918524</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How was Lal's species and gender chosen?</td>\n",
              "      <td>[I decided to allow Lal to choose its own appe...</td>\n",
              "      <td>I allowed Lal to choose its own appearance and...</td>\n",
              "      <td>Lal chose her own identity as a human female, ...</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.937802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.838316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What happened to Lal?</td>\n",
              "      <td>[Lal..., What do you feel, Lal?, Yes, Lal. I a...</td>\n",
              "      <td>Lal experienced a malfunction and had to be re...</td>\n",
              "      <td>Lal experienced a cascade failure in her neura...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.467402</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6db604b1-be49-45c5-a062-8224e8028416')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6db604b1-be49-45c5-a062-8224e8028416 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6db604b1-be49-45c5-a062-8224e8028416');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2954cf57-c9d3-4c46-bcab-03bca7f1b895\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2954cf57-c9d3-4c46-bcab-03bca7f1b895')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2954cf57-c9d3-4c46-bcab-03bca7f1b895 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Is Lal your daughter?\",\n          \"How many calculations per second can Lal complete?\",\n          \"What happened to Lal?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Yes, Lal is my daughter.\",\n          \"Lal's computational abilities are not explicitly defined in the context provided.\",\n          \"Lal experienced a malfunction and had to be returned to the lab for repairs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Yes, Lal is my daughter. I created Lal.\",\n          \"Lal is capable of completing sixty trillion calculations per second.\",\n          \"Lal experienced a cascade failure in her neural net, triggered by distress from her impending separation from me to Galor IV. I deactivated Lal once she suffered complete neural system failure.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3709846757342271,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6,\n          0.0,\n          0.6666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3939907815917057,\n        \"min\": 0.0,\n        \"max\": 0.9999988696653132,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9999988696653132,\n          0.0,\n          0.9999985764232017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2041241452319315,\n        \"min\": 0.0,\n        \"max\": 0.5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4704132870069684,\n        \"min\": 0.0,\n        \"max\": 0.9999999999,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.9999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.26115241262432315,\n        \"min\": 0.21414259722977252,\n        \"max\": 0.8383163160932917,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7446720226973956,\n          0.21414259722977252\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is currently a bug in ragas where faithfulness is not properly computed when the answer is \"I don't know,\" as it was in question 1 above."
      ],
      "metadata": {
        "id": "ujZ6Gk7etCwc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SGP4ZW3szuE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}